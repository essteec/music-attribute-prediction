{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e48cb19b",
   "metadata": {},
   "source": [
    "# Preprocessed Feature Files Analysis\n",
    "\n",
    "Analyzing all preprocessed `.npy` feature files to verify data quality and distributions.\n",
    "\n",
    "**Features analyzed:**\n",
    "- Audio features (23 including artist features)\n",
    "- Text statistics (5 features)\n",
    "- Sentiment (2 features)\n",
    "- Embeddings (384 features)\n",
    "- **Total: 414 features**\n",
    "\n",
    "**Experiment 2 includes:**\n",
    "- `log_total_artist_followers` (log-transformed)\n",
    "- `avg_artist_popularity` (0-100 scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ecc6e",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ad37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Academic publication style settings\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_context(\"paper\", font_scale=1.4)\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "# Configure matplotlib for publication quality\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif']\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['legend.frameon'] = True\n",
    "plt.rcParams['legend.edgecolor'] = 'black'\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['grid.linestyle'] = '--'\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "# Paths\n",
    "features_dir = Path('../features')\n",
    "output_dir = Path('../results/figures/feature_eda')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Feature names (UPDATED for Experiment 2 with artist features)\n",
    "AUDIO_FEATURE_NAMES = [\n",
    "    'acousticness', 'instrumentalness', 'liveness', 'speechiness',\n",
    "    'loudness', 'tempo', 'duration_ms', 'year', 'mode',\n",
    "    'key_sin', 'key_cos',\n",
    "    'genre_Blues', 'genre_Classical', 'genre_Country', 'genre_Electronic',\n",
    "    'genre_Folk', 'genre_Hip-Hop', 'genre_Jazz', 'genre_Pop', 'genre_R&B', 'genre_Rock',\n",
    "    'log_total_artist_followers', 'avg_artist_popularity'  # NEW: Artist features\n",
    "]\n",
    "\n",
    "TEXT_STAT_NAMES = [\n",
    "    'word_count', 'unique_word_count', 'unique_ratio', 'avg_word_length', 'char_count'\n",
    "]\n",
    "\n",
    "SENTIMENT_NAMES = [\n",
    "    'sentiment_polarity', 'sentiment_subjectivity'\n",
    "]\n",
    "\n",
    "print(\"Libraries imported and paths configured\")\n",
    "print(\"Academic publication style configured\")\n",
    "print(f\"  Features directory: {features_dir.absolute()}\")\n",
    "print(f\"  Output directory: {output_dir.absolute()}\")\n",
    "print(f\"\\nFeature names defined:\")\n",
    "print(f\"  Audio features: {len(AUDIO_FEATURE_NAMES)} (includes 2 artist features)\")\n",
    "print(f\"  Text statistics: {len(TEXT_STAT_NAMES)}\")\n",
    "print(f\"  Sentiment features: {len(SENTIMENT_NAMES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d69acf",
   "metadata": {},
   "source": [
    "## 2. Feature File Inventory\n",
    "\n",
    "List all available .npy and .pkl files in the features directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a287f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all feature files\n",
    "features_dir = Path(\"../features/\")\n",
    "npy_files = sorted(features_dir.glob(\"*.npy\"))\n",
    "pkl_files = sorted(features_dir.glob(\"*.pkl\"))\n",
    "\n",
    "print(f\"NumPy arrays (.npy): {len(npy_files)}\")\n",
    "print(\"=\" * 60)\n",
    "for f in npy_files:\n",
    "    size_mb = f.stat().st_size / (1024**2)\n",
    "    print(f\"  {f.name:<35} {size_mb:>7.2f} MB\")\n",
    "\n",
    "print(f\"\\nPickle files (.pkl): {len(pkl_files)}\")\n",
    "print(\"=\" * 60)\n",
    "for f in pkl_files:\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"  {f.name:<35} {size_kb:>7.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f8c83",
   "metadata": {},
   "source": [
    "## 3. Load All Features\n",
    "\n",
    "Load audio, text statistics, sentiment, embeddings, and target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "\n",
    "# Audio features\n",
    "try:\n",
    "    features['audio'] = {\n",
    "        'train': np.load(features_dir / 'X_train_audio.npy'),\n",
    "        'val': np.load(features_dir / 'X_val_audio.npy'),\n",
    "        'test': np.load(features_dir / 'X_test_audio.npy'),\n",
    "    }\n",
    "    print(f\"Audio features: {features['audio']['train'].shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Audio features not found\")\n",
    "\n",
    "# Text statistics\n",
    "try:\n",
    "    features['text_stats'] = {\n",
    "        'train': np.load(features_dir / 'X_train_text_stats.npy'),\n",
    "        'val': np.load(features_dir / 'X_val_text_stats.npy'),\n",
    "        'test': np.load(features_dir / 'X_test_text_stats.npy'),\n",
    "    }\n",
    "    print(f\"Text stats: {features['text_stats']['train'].shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Text statistics not found\")\n",
    "\n",
    "# Sentiment\n",
    "try:\n",
    "    features['sentiment'] = {\n",
    "        'train': np.load(features_dir / 'X_train_sentiment.npy'),\n",
    "        'val': np.load(features_dir / 'X_val_sentiment.npy'),\n",
    "        'test': np.load(features_dir / 'X_test_sentiment.npy'),\n",
    "    }\n",
    "    print(f\"Sentiment: {features['sentiment']['train'].shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Sentiment features not found\")\n",
    "\n",
    "# Embeddings\n",
    "try:\n",
    "    features['embeddings'] = {\n",
    "        'train': np.load(features_dir / 'X_train_embeddings.npy'),\n",
    "        'val': np.load(features_dir / 'X_val_embeddings.npy'),\n",
    "        'test': np.load(features_dir / 'X_test_embeddings.npy'),\n",
    "    }\n",
    "    print(f\"Embeddings: {features['embeddings']['train'].shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Embeddings not found (run: python run_preprocessing.py --steps embeddings)\")\n",
    "\n",
    "# Targets\n",
    "targets = {}\n",
    "target_names = ['valence', 'energy', 'danceability', 'popularity']\n",
    "for target in target_names:\n",
    "    try:\n",
    "        targets[target] = {\n",
    "            'train': np.load(features_dir / f'y_train_{target}.npy'),\n",
    "            'val': np.load(features_dir / f'y_val_{target}.npy'),\n",
    "            'test': np.load(features_dir / f'y_test_{target}.npy'),\n",
    "        }\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Target '{target}' not found\")\n",
    "\n",
    "print(f\"\\n{len(targets)} targets loaded: {list(targets.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44af6d0",
   "metadata": {},
   "source": [
    "## 4. Feature Dimensions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbd467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dimensions table\n",
    "dims_data = []\n",
    "for feat_name, feat_dict in features.items():\n",
    "    dims_data.append({\n",
    "        'Feature Type': feat_name,\n",
    "        'Train Shape': str(feat_dict['train'].shape),\n",
    "        'Val Shape': str(feat_dict['val'].shape),\n",
    "        'Test Shape': str(feat_dict['test'].shape),\n",
    "        'N Features': feat_dict['train'].shape[1],\n",
    "    })\n",
    "\n",
    "dims_df = pd.DataFrame(dims_data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TABLE 1: Feature Matrix Dimensions\")\n",
    "print(\"=\"*80)\n",
    "print(dims_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal features available: {dims_df['N Features'].sum()}\")\n",
    "print(f\"Training samples: {features[list(features.keys())[0]]['train'].shape[0]:,}\")\n",
    "print(f\"Validation samples: {features[list(features.keys())[0]]['val'].shape[0]:,}\")\n",
    "print(f\"Test samples: {features[list(features.keys())[0]]['test'].shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d46ee",
   "metadata": {},
   "source": [
    "## 5. Data Quality Checks\n",
    "\n",
    "Check for NaN values, infinite values, and compute basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df670ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_array_quality(arr: np.ndarray, name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Check array for common data quality issues.\"\"\"\n",
    "    return {\n",
    "        'Feature': name,\n",
    "        'Shape': str(arr.shape),\n",
    "        'NaN Count': np.isnan(arr).sum(),\n",
    "        'Inf Count': np.isinf(arr).sum(),\n",
    "        'Min': arr.min() if arr.size > 0 else None,\n",
    "        'Max': arr.max() if arr.size > 0 else None,\n",
    "        'Mean': arr.mean() if arr.size > 0 else None,\n",
    "        'Std': arr.std() if arr.size > 0 else None,\n",
    "    }\n",
    "\n",
    "quality_results = []\n",
    "for feat_name, feat_dict in features.items():\n",
    "    for split_name, arr in feat_dict.items():\n",
    "        result = check_array_quality(arr, f\"{feat_name}_{split_name}\")\n",
    "        quality_results.append(result)\n",
    "\n",
    "quality_df = pd.DataFrame(quality_results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TABLE 2: Data Quality Assessment\")\n",
    "print(\"=\"*80)\n",
    "print(quality_df[['Feature', 'NaN Count', 'Inf Count', 'Min', 'Max', 'Mean', 'Std']].to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for issues\n",
    "total_nan = quality_df['NaN Count'].sum()\n",
    "total_inf = quality_df['Inf Count'].sum()\n",
    "\n",
    "if total_nan > 0:\n",
    "    print(f\"\\nWARNING: {total_nan} NaN values detected!\")\n",
    "else:\n",
    "    print(f\"\\nNo NaN values detected\")\n",
    "    \n",
    "if total_inf > 0:\n",
    "    print(f\"WARNING: {total_inf} Inf values detected!\")\n",
    "else:\n",
    "    print(f\"No Inf values detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d8b6c",
   "metadata": {},
   "source": [
    "## 6. Feature Distributions\n",
    "\n",
    "Visualize distributions for each feature type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3807be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature names mapping\n",
    "feature_names_map = {\n",
    "    'audio': AUDIO_FEATURE_NAMES,\n",
    "    'text_stats': TEXT_STAT_NAMES,\n",
    "    'sentiment': SENTIMENT_NAMES,\n",
    "    'embeddings': [f'emb_{i}' for i in range(384)]  # 384-d embeddings\n",
    "}\n",
    "\n",
    "for feat_name, feat_dict in features.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{feat_name.upper()} - Training Set Statistics\")\n",
    "    print('='*80)\n",
    "    \n",
    "    X = feat_dict['train']\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = feature_names_map.get(feat_name, [f'feature_{i}' for i in range(n_features)])\n",
    "    \n",
    "    # Summary statistics per feature\n",
    "    stats = pd.DataFrame({\n",
    "        'feature': feature_names[:n_features],\n",
    "        'mean': X.mean(axis=0),\n",
    "        'std': X.std(axis=0),\n",
    "        'min': X.min(axis=0),\n",
    "        'max': X.max(axis=0),\n",
    "        'median': np.median(X, axis=0),\n",
    "    })\n",
    "    \n",
    "    print(f\"Features: {n_features}\")\n",
    "    print(f\"Mean range: [{stats['mean'].min():.4f}, {stats['mean'].max():.4f}]\")\n",
    "    print(f\"Std range:  [{stats['std'].min():.4f}, {stats['std'].max():.4f}]\")\n",
    "    \n",
    "    # Display first 30 features statistics\n",
    "    if n_features <= 30:\n",
    "        print(f\"\\nFirst {min(30, n_features)} features:\")\n",
    "        print(stats.head(30).to_string(index=False))\n",
    "    \n",
    "    # Plot distribution of first few features (if not too many)\n",
    "    if n_features <= 50:\n",
    "        n_plots = min(6, n_features)\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i in range(n_plots):\n",
    "            ax = axes[i]\n",
    "            ax.hist(X[:, i], bins=50, alpha=0.8, edgecolor='black', \n",
    "                   color='steelblue', linewidth=1.2)\n",
    "            feature_label = feature_names[i] if i < len(feature_names) else f'Feature {i}'\n",
    "            ax.set_title(feature_label, fontsize=17, fontweight='bold', pad=10)\n",
    "            ax.set_xlabel('Value', fontsize=15, fontweight='bold')\n",
    "            ax.set_ylabel('Frequency', fontsize=15, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.tick_params(axis='y', labelsize=12)\n",
    "            ax.tick_params(axis='x', labelsize=12)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(n_plots, 6):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        fig.suptitle(f'{feat_name.upper()} - Feature Distributions', \n",
    "                    fontsize=18, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / f'{feat_name}_distributions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Saved to {feat_name}_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b7c3e5",
   "metadata": {},
   "source": [
    "## 7. Target Distributions\n",
    "\n",
    "Compare train/val/test distributions for all target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52cc820",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Target Variables: Train/Validation/Test Distribution Comparison', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "colors = {'train': '#3498db', 'val': '#e74c3c', 'test': '#2ecc71'}\n",
    "\n",
    "for idx, (target_name, target_dict) in enumerate(targets.items()):\n",
    "    y_train = target_dict['train']\n",
    "    y_val = target_dict['val']\n",
    "    y_test = target_dict['test']\n",
    "    \n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Overlapping histograms\n",
    "    ax.hist(y_train, bins=50, alpha=0.6, label='Train', edgecolor='black', \n",
    "            color=colors['train'], linewidth=1.2)\n",
    "    ax.hist(y_val, bins=50, alpha=0.6, label='Validation', edgecolor='black', \n",
    "            color=colors['val'], linewidth=1.2)\n",
    "    ax.hist(y_test, bins=50, alpha=0.6, label='Test', edgecolor='black', \n",
    "            color=colors['test'], linewidth=1.2)\n",
    "    \n",
    "    ax.set_title(f'{target_name.capitalize()}', fontsize=15, fontweight='bold', pad=12)\n",
    "    ax.set_xlabel('Value', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='best', frameon=True, shadow=True, edgecolor='black')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{target_name.upper()}:\")\n",
    "    print(f\"  Train: μ={y_train.mean():.4f}, σ={y_train.std():.4f}, \"\n",
    "          f\"range=[{y_train.min():.4f}, {y_train.max():.4f}]\")\n",
    "    print(f\"  Val:   μ={y_val.mean():.4f}, σ={y_val.std():.4f}, \"\n",
    "          f\"range=[{y_val.min():.4f}, {y_val.max():.4f}]\")\n",
    "    print(f\"  Test:  μ={y_test.mean():.4f}, σ={y_test.std():.4f}, \"\n",
    "          f\"range=[{y_test.min():.4f}, {y_test.max():.4f}]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'target_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nSaved to target_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29551927",
   "metadata": {},
   "source": [
    "## 8. Scalers and Transformers Inspection\n",
    "\n",
    "Examine saved scaler/transformer objects and their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404747cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_files = {\n",
    "    'audio_scaler': 'audio_scaler.pkl',\n",
    "    'text_stats_scaler': 'text_stats_scaler.pkl',\n",
    "    'sentiment_scaler': 'sentiment_scaler.pkl',\n",
    "    'audio_power_transformer': 'audio_power_transformer.pkl',\n",
    "}\n",
    "\n",
    "print(\"Scalers and Transformers\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for scaler_name, filename in scaler_files.items():\n",
    "    filepath = features_dir / filename\n",
    "    if filepath.exists():\n",
    "        scaler = joblib.load(filepath)\n",
    "        print(f\"\\n{scaler_name}:\")\n",
    "        print(f\"  Type: {type(scaler).__name__}\")\n",
    "        \n",
    "        if hasattr(scaler, 'mean_'):\n",
    "            print(f\"  Mean (first 5): {scaler.mean_[:5]}\")\n",
    "        if hasattr(scaler, 'scale_'):\n",
    "            print(f\"  Scale (first 5): {scaler.scale_[:5]}\")\n",
    "        if hasattr(scaler, 'var_'):\n",
    "            print(f\"  Variance (first 5): {scaler.var_[:5]}\")\n",
    "        if hasattr(scaler, 'n_features_in_'):\n",
    "            print(f\"  N Features: {scaler.n_features_in_}\")\n",
    "    else:\n",
    "        print(f\"\\n{scaler_name}: Not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e40676",
   "metadata": {},
   "source": [
    "## 9. Correlation Analysis\n",
    "\n",
    "Compute correlation matrices and identify highly correlated feature pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed69d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all non-genre features into a single correlation matrix\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMBINED FEATURE CORRELATION ANALYSIS (excluding genres)\")\n",
    "print('='*80)\n",
    "\n",
    "# Collect all features (excluding genre columns from audio)\n",
    "combined_features = []\n",
    "combined_names = []\n",
    "\n",
    "for feat_name, feat_dict in features.items():\n",
    "    X = feat_dict['train']\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = feature_names_map.get(feat_name, [f'feature_{i}' for i in range(n_features)])\n",
    "    \n",
    "    if feat_name == 'audio':\n",
    "        # Filter out genre features (indices 11-20)\n",
    "        non_genre_mask = [i for i in range(n_features) if not feature_names[i].startswith('genre_')]\n",
    "        X_filtered = X[:, non_genre_mask]\n",
    "        filtered_names = [feature_names[i] for i in non_genre_mask]\n",
    "        combined_features.append(X_filtered)\n",
    "        combined_names.extend(filtered_names)\n",
    "        print(f\"Audio features (non-genre): {X_filtered.shape[1]} features\")\n",
    "    elif feat_name != 'embeddings':  # Skip embeddings (too high-dimensional)\n",
    "        combined_features.append(X)\n",
    "        combined_names.extend(feature_names[:n_features])\n",
    "        print(f\"{feat_name.capitalize()}: {n_features} features\")\n",
    "# Concatenate all features\n",
    "X_combined = np.concatenate(combined_features, axis=1)\n",
    "print(f\"\\nCombined feature matrix: {X_combined.shape}\")\n",
    "print(f\"Total features in correlation matrix: {len(combined_names)}\")\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr = np.corrcoef(X_combined.T)\n",
    "\n",
    "# Create publication-quality heatmap\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "\n",
    "sns.heatmap(corr, cmap='RdBu_r', center=0, \n",
    "            square=True, linewidths=0.8, linecolor='white',\n",
    "            cbar_kws={\"shrink\": 0.75, \"label\": \"Pearson Correlation (r)\"},\n",
    "            annot=True, fmt='.2f', annot_kws={\"fontsize\": 7, \"weight\": \"bold\"},\n",
    "            vmin=-1, vmax=1,\n",
    "            xticklabels=combined_names,\n",
    "            yticklabels=combined_names, ax=ax)\n",
    "\n",
    "ax.set_title('Combined Feature Correlation Matrix (Audio + Text + Sentiment)', \n",
    "             fontsize=18, fontweight='bold', pad=20)\n",
    "\n",
    "# Rotate labels for readability\n",
    "plt.xticks(rotation=45, ha='right', fontsize=9, fontweight='bold')\n",
    "plt.yticks(rotation=0, fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'combined_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nSaved to combined_correlation_matrix.png\")\n",
    "\n",
    "# Find highly correlated pairs\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"HIGHLY CORRELATED FEATURE PAIRS (|r| > 0.8)\")\n",
    "print('='*80)\n",
    "\n",
    "high_corr = []\n",
    "for i in range(len(combined_names)):\n",
    "    for j in range(i+1, len(combined_names)):\n",
    "        if abs(corr[i, j]) > 0.8:\n",
    "            high_corr.append((combined_names[i], combined_names[j], corr[i, j]))\n",
    "\n",
    "if high_corr:\n",
    "    # Sort by absolute correlation value\n",
    "    high_corr.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    print(f\"Found {len(high_corr)} highly correlated pairs:\\n\")\n",
    "    for feat_i, feat_j, r in high_corr:\n",
    "        print(f\"  {feat_i:25s} ↔ {feat_j:25s}: r = {r:+.3f}\")\n",
    "else:\n",
    "    print(\"No highly correlated pairs found (|r| > 0.8)\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CORRELATION MATRIX STATISTICS\")\n",
    "print('='*80)\n",
    "upper_tri_corr = corr[np.triu_indices_from(corr, k=1)]\n",
    "print(f\"  Mean absolute correlation: {np.mean(np.abs(upper_tri_corr)):.3f}\")\n",
    "print(f\"  Median absolute correlation: {np.median(np.abs(upper_tri_corr)):.3f}\")\n",
    "print(f\"  Max correlation: {np.max(upper_tri_corr):.3f}\")\n",
    "print(f\"  Min correlation: {np.min(upper_tri_corr):.3f}\")\n",
    "print(f\"  Pairs with |r| > 0.8: {len(high_corr)}\")\n",
    "print(f\"  Pairs with |r| > 0.5: {(np.abs(upper_tri_corr) > 0.5).sum()}\")\n",
    "print(f\"  Pairs with |r| > 0.3: {(np.abs(upper_tri_corr) > 0.3).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983f967",
   "metadata": {},
   "source": [
    "## 9b. Feature Importance by Variance\n",
    "\n",
    "Analyze feature importance based on variance (useful for dimensionality assessment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa93b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze variance for each feature type\n",
    "for feat_name, feat_dict in features.items():\n",
    "    X = feat_dict['train']\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = feature_names_map.get(feat_name, [f'feature_{i}' for i in range(n_features)])\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{feat_name.upper()} - Variance Analysis\")\n",
    "    print('='*80)\n",
    "    \n",
    "    # Compute variance\n",
    "    variances = X.var(axis=0)\n",
    "    \n",
    "    # Create variance dataframe\n",
    "    var_df = pd.DataFrame({\n",
    "        'feature': feature_names[:n_features],\n",
    "        'variance': variances,\n",
    "    }).sort_values('variance', ascending=False)\n",
    "    \n",
    "    print(f\"Variance range: [{variances.min():.6f}, {variances.max():.6f}]\")\n",
    "    print(f\"Mean variance: {variances.mean():.6f}\")\n",
    "    \n",
    "    # Show top variance features\n",
    "    if n_features <= 30:\n",
    "        print(f\"\\nTop 10 features by variance:\")\n",
    "        print(var_df.head(10).to_string(index=False))\n",
    "        \n",
    "        # Plot variance - Academic style\n",
    "        fig, ax = plt.subplots(figsize=(14, 7))\n",
    "        bars = ax.bar(range(len(variances)), var_df['variance'].values, \n",
    "                     color='steelblue', alpha=0.75, edgecolor='black', linewidth=1.2)\n",
    "        \n",
    "        ax.set_xlabel('Feature (ranked by variance)', fontsize=13, fontweight='bold')\n",
    "        ax.set_ylabel('Variance', fontsize=13, fontweight='bold')\n",
    "        ax.set_title(f'{feat_name.upper()} - Feature Variance Ranking', \n",
    "                    fontsize=16, fontweight='bold', pad=15)\n",
    "        ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        \n",
    "        # Highlight top 3\n",
    "        for i in range(min(3, len(bars))):\n",
    "            bars[i].set_color('#e74c3c')\n",
    "            bars[i].set_alpha(0.9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / f'{feat_name}_variance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Saved to {feat_name}_variance.png\")\n",
    "    else:\n",
    "        # For high-dimensional features (embeddings)\n",
    "        low_var_count = (variances < 0.01).sum()\n",
    "        print(f\"Low variance features (var < 0.01): {low_var_count} ({low_var_count/n_features*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE FILES EDA - SUMMARY REPORT (EXPERIMENT 2)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_features = sum(feat_dict['train'].shape[1] for feat_dict in features.values())\n",
    "total_samples_train = list(features.values())[0]['train'].shape[0] if features else 0\n",
    "\n",
    "print(f\"\\nFEATURE INVENTORY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Total features: {total_features} ← Experiment 2 (includes artist features)\")\n",
    "for feat_name, feat_dict in features.items():\n",
    "    n_feat = feat_dict['train'].shape[1]\n",
    "    pct = (n_feat / total_features * 100) if total_features > 0 else 0\n",
    "    print(f\"  • {feat_name.capitalize():15s}: {n_feat:3d} features ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nNEW FEATURES FOR EXPERIMENT 2:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Audio features: 21 → 23 (+2 artist features)\")\n",
    "print(f\"    • log_total_artist_followers (log-transformed)\")\n",
    "print(f\"    • avg_artist_popularity (0-100 scale)\")\n",
    "print(f\"  Total features: 412 → 414 (+2)\")\n",
    "\n",
    "print(f\"\\nDATASET SPLITS\")\n",
    "print(f\"{'='*80}\")\n",
    "if features:\n",
    "    train_size = list(features.values())[0]['train'].shape[0]\n",
    "    val_size = list(features.values())[0]['val'].shape[0]\n",
    "    test_size = list(features.values())[0]['test'].shape[0]\n",
    "    total_size = train_size + val_size + test_size\n",
    "    \n",
    "    print(f\"  Training:   {train_size:7,} samples ({train_size/total_size*100:5.1f}%)\")\n",
    "    print(f\"  Validation: {val_size:7,} samples ({val_size/total_size*100:5.1f}%)\")\n",
    "    print(f\"  Test:       {test_size:7,} samples ({test_size/total_size*100:5.1f}%)\")\n",
    "    print(f\"  {'─'*80}\")\n",
    "    print(f\"  Total:      {total_size:7,} samples (100.0%)\")\n",
    "\n",
    "print(f\"\\nFILES PROCESSED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  NumPy arrays (.npy): {len(npy_files)}\")\n",
    "print(f\"  Pickle files (.pkl): {len(pkl_files)}\")\n",
    "\n",
    "print(f\"\\nVISUALIZATIONS GENERATED\")\n",
    "print(f\"{'='*80}\")\n",
    "viz_files = sorted(output_dir.glob(\"*.png\"))\n",
    "print(f\"  Total: {len(viz_files)} publication-quality figures\")\n",
    "for vf in viz_files:\n",
    "    print(f\"  • {vf.name}\")\n",
    "\n",
    "print(f\"\\nEXPERIMENT 2 STATUS:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Artist features preprocessed and scaled\")\n",
    "print(f\"  New splits created from songs.csv\")\n",
    "print(f\"  414 total features ready for model training\")\n",
    "print(f\"  Next: Train enhanced models with artist features\")\n",
    "\n",
    "print(f\"\\nAll visualizations saved to: {output_dir.absolute()}\")\n",
    "print(f\"Resolution: 300 DPI (publication quality)\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
