{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93aef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Academic publication style with larger fonts\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman', 'DejaVu Serif', 'serif'],\n",
    "    'font.size': 13,\n",
    "    'axes.titlesize': 15,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'figure.titlesize': 16,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': False,\n",
    "    'grid.alpha': 0.3,\n",
    "    'axes.linewidth': 1.2,\n",
    "    'xtick.major.width': 1.2,\n",
    "    'ytick.major.width': 1.2,\n",
    "})\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results' / 'metrics' / 'experiment2_with_artist'\n",
    "FIGURES_DIR = PROJECT_ROOT / 'results' / 'figures'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Figures will be saved to: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7e6ab",
   "metadata": {},
   "source": [
    "## 1. Load Test Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda68b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent test evaluation results\n",
    "test_results_files = sorted(RESULTS_DIR.glob('test_evaluation_final_*.csv'))\n",
    "\n",
    "if not test_results_files:\n",
    "    raise FileNotFoundError(\"No test evaluation results found. Run test_evaluation_final.py first.\")\n",
    "\n",
    "# Load most recent results\n",
    "results_file = test_results_files[-1]\n",
    "df_test = pd.read_csv(results_file)\n",
    "\n",
    "print(f\"Loaded: {results_file.name}\")\n",
    "print(f\"Shape: {df_test.shape}\")\n",
    "print(f\"\\nColumns: {df_test.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=\"*80)\n",
    "print(\"TEST EVALUATION OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal evaluations: {len(df_test)}\")\n",
    "print(f\"Targets evaluated: {df_test['target'].unique().tolist()}\")\n",
    "print(f\"Models evaluated: {sorted(df_test['model'].unique().tolist())}\")\n",
    "print(f\"Model sources: {df_test['model_source'].unique().tolist()}\")\n",
    "print(f\"\\nTest samples: {df_test['n_samples'].iloc[0]:,}\")\n",
    "\n",
    "print(\"\\nEvaluations by model source:\")\n",
    "print(df_test['model_source'].value_counts())\n",
    "\n",
    "print(\"\\nEvaluations by target:\")\n",
    "print(df_test['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e29f0",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL PERFORMANCE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_stats = df_test.groupby('model_source')[['r2', 'rmse', 'mae']].agg(['mean', 'std', 'min', 'max'])\n",
    "print(\"\\nAcross all models and targets:\")\n",
    "print(summary_stats.round(4))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE BY TARGET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_summary = df_test.groupby(['target', 'model_source'])[['r2', 'rmse', 'mae']].mean().round(4)\n",
    "print(target_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21dd160",
   "metadata": {},
   "source": [
    "## 3. Best Models per Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model for each target (overall)\n",
    "print(\"=\"*80)\n",
    "print(\"BEST MODELS PER TARGET (HIGHEST R²)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_models_overall = df_test.loc[df_test.groupby('target')['r2'].idxmax()]\n",
    "best_display = best_models_overall[[\n",
    "    'target', 'model', 'model_source', 'num_features', 'r2', 'rmse', 'mae'\n",
    "]].sort_values('r2', ascending=False)\n",
    "\n",
    "print(best_display.to_string(index=False))\n",
    "\n",
    "# Best enhanced vs best RFE per target\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST ENHANCED VS BEST RFE PER TARGET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for target in sorted(df_test['target'].unique()):\n",
    "    target_df = df_test[df_test['target'] == target]\n",
    "    \n",
    "    # Best enhanced\n",
    "    enhanced_df = target_df[target_df['model_source'] == 'enhanced']\n",
    "    if len(enhanced_df) > 0:\n",
    "        best_enhanced = enhanced_df.loc[enhanced_df['r2'].idxmax()]\n",
    "    \n",
    "    # Best RFE\n",
    "    rfe_df = target_df[target_df['model_source'] == 'rfe']\n",
    "    if len(rfe_df) > 0:\n",
    "        best_rfe = rfe_df.loc[rfe_df['r2'].idxmax()]\n",
    "    \n",
    "    print(f\"\\n{target.upper()}:\")\n",
    "    if len(enhanced_df) > 0:\n",
    "        print(f\"  Enhanced ({int(best_enhanced['num_features'])} features):\")\n",
    "        print(f\"    {best_enhanced['model']:20s} R²={best_enhanced['r2']:.4f}, RMSE={best_enhanced['rmse']:.4f}\")\n",
    "    \n",
    "    if len(rfe_df) > 0:\n",
    "        print(f\"  RFE ({int(best_rfe['num_features'])} features):\")\n",
    "        print(f\"    {best_rfe['model']:20s} R²={best_rfe['r2']:.4f}, RMSE={best_rfe['rmse']:.4f}\")\n",
    "    \n",
    "    if len(enhanced_df) > 0 and len(rfe_df) > 0:\n",
    "        r2_diff = best_rfe['r2'] - best_enhanced['r2']\n",
    "        feature_reduction = (1 - best_rfe['num_features'] / best_enhanced['num_features']) * 100\n",
    "        print(f\"  Comparison:\")\n",
    "        print(f\"    ΔR² = {r2_diff:+.4f} ({(r2_diff/best_enhanced['r2']*100):+.2f}%)\")\n",
    "        print(f\"    Feature reduction: {feature_reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac2a13",
   "metadata": {},
   "source": [
    "## 4. Enhanced vs RFE Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bec547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R² comparison - Enhanced vs RFE (grouped by target)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "axes = axes.flatten()\n",
    "\n",
    "targets = sorted(df_test['target'].unique())\n",
    "colors_enhanced = '#3498db'\n",
    "colors_rfe = '#e74c3c'\n",
    "\n",
    "for idx, target in enumerate(targets):\n",
    "    target_df = df_test[df_test['target'] == target]\n",
    "    \n",
    "    # Get unique models\n",
    "    models = sorted(target_df['model'].unique())\n",
    "    \n",
    "    # Prepare data\n",
    "    enhanced_r2 = []\n",
    "    rfe_r2 = []\n",
    "    \n",
    "    for model in models:\n",
    "        model_df = target_df[target_df['model'] == model]\n",
    "        \n",
    "        enhanced_val = model_df[model_df['model_source'] == 'enhanced']['r2'].values\n",
    "        enhanced_r2.append(enhanced_val[0] if len(enhanced_val) > 0 else 0)\n",
    "        \n",
    "        rfe_val = model_df[model_df['model_source'] == 'rfe']['r2'].values\n",
    "        rfe_r2.append(rfe_val[0] if len(rfe_val) > 0 else 0)\n",
    "    \n",
    "    # Plot grouped bars\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.38\n",
    "    \n",
    "    bars1 = axes[idx].bar(x - width/2, enhanced_r2, width, label='Enhanced (414 features)',\n",
    "                         color=colors_enhanced, alpha=0.85, edgecolor='black', linewidth=1)\n",
    "    bars2 = axes[idx].bar(x + width/2, rfe_r2, width, label='RFE (reduced features)',\n",
    "                         color=colors_rfe, alpha=0.85, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    axes[idx].set_ylabel('R² Score', fontweight='bold')\n",
    "    axes[idx].set_title(f'{target.capitalize()}', fontweight='bold', fontsize=16, pad=10)\n",
    "    axes[idx].set_xticks(x)\n",
    "    axes[idx].set_xticklabels(models, rotation=45, ha='right', fontsize=10)\n",
    "    axes[idx].legend(loc='lower right', frameon=True, fancybox=False, \n",
    "                    edgecolor='black', framealpha=0.95)\n",
    "    axes[idx].grid(True, alpha=0.25, linestyle='--', axis='y')\n",
    "    axes[idx].set_ylim([0, max(max(enhanced_r2), max(rfe_r2)) * 1.15])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "        if enhanced_r2[i] > 0:\n",
    "            axes[idx].text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 0.01,\n",
    "                          f'{enhanced_r2[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        if rfe_r2[i] > 0:\n",
    "            axes[idx].text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 0.01,\n",
    "                          f'{rfe_r2[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.suptitle('Test Set Performance: Enhanced vs RFE Models (R² Comparison)',\n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'test_r2_enhanced_vs_rfe_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: test_r2_enhanced_vs_rfe_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a279d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R² visualization - enhanced models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, target in enumerate(targets):\n",
    "    target_df = df_test[df_test['target'] == target]\n",
    "    enhanced_df = target_df[target_df['model_source'] == 'enhanced']\n",
    "    \n",
    "    if len(enhanced_df) > 0:\n",
    "        # Sort by R² score\n",
    "        enhanced_sorted = enhanced_df.sort_values('r2', ascending=True)\n",
    "        models = enhanced_sorted['model'].values\n",
    "        r2_values = enhanced_sorted['r2'].values\n",
    "        \n",
    "        # Color by tuning status\n",
    "        colors = ['#27ae60' if '_tuned' in m else '#e74c3c' for m in models]\n",
    "        \n",
    "        bars = axes[idx].barh(range(len(models)), r2_values, color=colors,\n",
    "                             alpha=0.85, edgecolor='black', linewidth=1.2)\n",
    "        \n",
    "        axes[idx].set_yticks(range(len(models)))\n",
    "        axes[idx].set_yticklabels(models, fontsize=13)\n",
    "        axes[idx].set_xlabel('R² Score', fontweight='bold')\n",
    "        axes[idx].set_title(f'{target.capitalize()}',\n",
    "                           fontweight='bold', fontsize=16, pad=10)\n",
    "        axes[idx].grid(True, alpha=0.25, linestyle='--', axis='x')\n",
    "        axes[idx].set_xlim([0, max(r2_values) * 1.15])\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, val) in enumerate(zip(bars, r2_values)):\n",
    "            axes[idx].text(val + 0.008, i, f'{val:.4f}', va='center',\n",
    "                          fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#e74c3c', edgecolor='black', label='Default', alpha=0.85),\n",
    "    Patch(facecolor='#27ae60', edgecolor='black', label='Tuned', alpha=0.85)\n",
    "]\n",
    "if len(enhanced_df) > 0:\n",
    "    axes[0].legend(handles=legend_elements, loc='upper left', frameon=True,\n",
    "                  fancybox=False, edgecolor='black', framealpha=0.95)\n",
    "\n",
    "plt.suptitle('Test Set Performance',\n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'test_r2_enhanced_models.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: test_r2_enhanced_models.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE visualization - enhanced models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, target in enumerate(targets):\n",
    "    target_df = df_test[df_test['target'] == target]\n",
    "    enhanced_df = target_df[target_df['model_source'] == 'enhanced']\n",
    "    \n",
    "    if len(enhanced_df) > 0:\n",
    "        # Sort by RMSE (ascending = best first)\n",
    "        enhanced_sorted = enhanced_df.sort_values('rmse', ascending=False)\n",
    "        models = enhanced_sorted['model'].values\n",
    "        rmse_values = enhanced_sorted['rmse'].values\n",
    "        \n",
    "        # Color by tuning status\n",
    "        colors = ['#27ae60' if '_tuned' in m else '#e74c3c' for m in models]\n",
    "        \n",
    "        bars = axes[idx].barh(range(len(models)), rmse_values, color=colors,\n",
    "                             alpha=0.85, edgecolor='black', linewidth=1.2)\n",
    "        \n",
    "        axes[idx].set_yticks(range(len(models)))\n",
    "        axes[idx].set_yticklabels(models, fontsize=13)\n",
    "        axes[idx].set_xlabel('RMSE', fontweight='bold')\n",
    "        axes[idx].set_title(f'{target.capitalize()}',\n",
    "                           fontweight='bold', fontsize=16, pad=10)\n",
    "        axes[idx].grid(True, alpha=0.25, linestyle='--', axis='x')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, val) in enumerate(zip(bars, rmse_values)):\n",
    "            axes[idx].text(val + 0.003, i, f'{val:.4f}', va='center',\n",
    "                          fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#e74c3c', edgecolor='black', label='Default', alpha=0.85),\n",
    "    Patch(facecolor='#27ae60', edgecolor='black', label='Tuned', alpha=0.85)\n",
    "]\n",
    "if len(enhanced_df) > 0:\n",
    "    axes[0].legend(handles=legend_elements, loc='upper left', frameon=True,\n",
    "                  fancybox=False, edgecolor='black', framealpha=0.95)\n",
    "\n",
    "plt.suptitle('Test Set RMSE Performance',\n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'test_rmse_enhanced_models.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: test_rmse_enhanced_models.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4668b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R² visualization - RFE models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, target in enumerate(targets):\n",
    "    target_df = df_test[df_test['target'] == target]\n",
    "    rfe_df = target_df[target_df['model_source'] == 'rfe']\n",
    "    \n",
    "    if len(rfe_df) > 0:\n",
    "        # Sort by R² score\n",
    "        rfe_sorted = rfe_df.sort_values('r2', ascending=True)\n",
    "        models = rfe_sorted['model'].values\n",
    "        r2_values = rfe_sorted['r2'].values\n",
    "        num_features = rfe_sorted['num_features'].values\n",
    "        \n",
    "        # Color by tuning status\n",
    "        colors = ['#27ae60' if '_tuned' in m else '#e74c3c' for m in models]\n",
    "        \n",
    "        bars = axes[idx].barh(range(len(models)), r2_values, color=colors,\n",
    "                             alpha=0.85, edgecolor='black', linewidth=1.2)\n",
    "        \n",
    "        axes[idx].set_yticks(range(len(models)))\n",
    "        axes[idx].set_yticklabels(models, fontsize=13)\n",
    "        axes[idx].set_xlabel('R² Score', fontweight='bold')\n",
    "        axes[idx].set_title(f'{target.capitalize()} ({int(num_features[0])} features)',\n",
    "                           fontweight='bold', fontsize=16, pad=10)\n",
    "        axes[idx].grid(True, alpha=0.25, linestyle='--', axis='x')\n",
    "        axes[idx].set_xlim([0, max(r2_values) * 1.15])\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, val) in enumerate(zip(bars, r2_values)):\n",
    "            axes[idx].text(val + 0.008, i, f'{val:.4f}', va='center',\n",
    "                          fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#e74c3c', edgecolor='black', label='Default', alpha=0.85),\n",
    "    Patch(facecolor='#27ae60', edgecolor='black', label='Tuned', alpha=0.85)\n",
    "]\n",
    "if len(rfe_df) > 0:\n",
    "    axes[0].legend(handles=legend_elements, loc='upper left', frameon=True,\n",
    "                  fancybox=False, edgecolor='black', framealpha=0.95)\n",
    "\n",
    "plt.suptitle('Test Set Performance: Feature Selected',\n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'test_r2_rfe_models.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: test_r2_rfe_models.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE visualization - RFE models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, target in enumerate(targets):\n",
    "    target_df = df_test[df_test['target'] == target]\n",
    "    rfe_df = target_df[target_df['model_source'] == 'rfe']\n",
    "    \n",
    "    if len(rfe_df) > 0:\n",
    "        # Sort by RMSE (ascending = best first)\n",
    "        rfe_sorted = rfe_df.sort_values('rmse', ascending=False)\n",
    "        models = rfe_sorted['model'].values\n",
    "        rmse_values = rfe_sorted['rmse'].values\n",
    "        num_features = rfe_sorted['num_features'].values\n",
    "        \n",
    "        # Color by tuning status\n",
    "        colors = ['#27ae60' if '_tuned' in m else '#e74c3c' for m in models]\n",
    "        \n",
    "        bars = axes[idx].barh(range(len(models)), rmse_values, color=colors,\n",
    "                             alpha=0.85, edgecolor='black', linewidth=1.2)\n",
    "        \n",
    "        axes[idx].set_yticks(range(len(models)))\n",
    "        axes[idx].set_yticklabels(models, fontsize=13)\n",
    "        axes[idx].set_xlabel('RMSE', fontweight='bold')\n",
    "        axes[idx].set_title(f'{target.capitalize()} ({int(num_features[0])} features)',\n",
    "                           fontweight='bold', fontsize=16, pad=10)\n",
    "        axes[idx].grid(True, alpha=0.25, linestyle='--', axis='x')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, val) in enumerate(zip(bars, rmse_values)):\n",
    "            axes[idx].text(val + 0.003, i, f'{val:.4f}', va='center',\n",
    "                          fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#e74c3c', edgecolor='black', label='Default', alpha=0.85),\n",
    "    Patch(facecolor='#27ae60', edgecolor='black', label='Tuned', alpha=0.85)\n",
    "]\n",
    "if len(rfe_df) > 0:\n",
    "    axes[0].legend(handles=legend_elements, loc='upper left', frameon=True,\n",
    "                  fancybox=False, edgecolor='black', framealpha=0.95)\n",
    "\n",
    "plt.suptitle('Test Set RMSE Performance: Feature Selection',\n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'test_rmse_rfe_models.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: test_rmse_rfe_models.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE comparison - Enhanced vs RFE\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, target in enumerate(targets):\n",
    "    target_df = df_test[df_test['target'] == target]\n",
    "    models = sorted(target_df['model'].unique())\n",
    "    \n",
    "    # Prepare data\n",
    "    enhanced_rmse = []\n",
    "    rfe_rmse = []\n",
    "    \n",
    "    for model in models:\n",
    "        model_df = target_df[target_df['model'] == model]\n",
    "        \n",
    "        enhanced_val = model_df[model_df['model_source'] == 'enhanced']['rmse'].values\n",
    "        enhanced_rmse.append(enhanced_val[0] if len(enhanced_val) > 0 else 0)\n",
    "        \n",
    "        rfe_val = model_df[model_df['model_source'] == 'rfe']['rmse'].values\n",
    "        rfe_rmse.append(rfe_val[0] if len(rfe_val) > 0 else 0)\n",
    "    \n",
    "    # Plot grouped bars\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.38\n",
    "    \n",
    "    bars1 = axes[idx].bar(x - width/2, enhanced_rmse, width, label='Enhanced (414 features)',\n",
    "                         color=colors_enhanced, alpha=0.85, edgecolor='black', linewidth=1)\n",
    "    bars2 = axes[idx].bar(x + width/2, rfe_rmse, width, label='RFE (reduced features)',\n",
    "                         color=colors_rfe, alpha=0.85, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    axes[idx].set_ylabel('RMSE (Lower is Better)', fontweight='bold')\n",
    "    axes[idx].set_title(f'{target.capitalize()}', fontweight='bold', fontsize=16, pad=10)\n",
    "    axes[idx].set_xticks(x)\n",
    "    axes[idx].set_xticklabels(models, rotation=45, ha='right', fontsize=10)\n",
    "    axes[idx].legend(loc='upper right', frameon=True, fancybox=False,\n",
    "                    edgecolor='black', framealpha=0.95)\n",
    "    axes[idx].grid(True, alpha=0.25, linestyle='--', axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "        if enhanced_rmse[i] > 0:\n",
    "            axes[idx].text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 0.003,\n",
    "                          f'{enhanced_rmse[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        if rfe_rmse[i] > 0:\n",
    "            axes[idx].text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 0.003,\n",
    "                          f'{rfe_rmse[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.suptitle('Test Set Performance: Enhanced vs RFE Models (RMSE Comparison)',\n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'test_rmse_enhanced_vs_rfe_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: test_rmse_enhanced_vs_rfe_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96709c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE models heatmap\n",
    "rfe_df = df_test[df_test['model_source'] == 'rfe']\n",
    "if len(rfe_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    pivot_rfe = rfe_df.pivot(index='model', columns='target', values='r2')\n",
    "    pivot_rfe = pivot_rfe[targets]\n",
    "    pivot_rfe = pivot_rfe.sort_values(pivot_rfe.columns.tolist(), ascending=False)\n",
    "    \n",
    "    sns.heatmap(pivot_rfe, annot=True, fmt='.4f', cmap='RdYlGn', center=0.35,\n",
    "                ax=ax, linewidths=1.5, vmin=0, vmax=0.9, cbar_kws={'label': 'R² Score'},\n",
    "                annot_kws={'size': 12, 'weight': 'bold'})\n",
    "    ax.set_title('RFE Models (34-394 features per target)', fontsize=18, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Target Variable', fontsize=15, fontweight='bold')\n",
    "    ax.set_ylabel('Model', fontsize=15, fontweight='bold')\n",
    "    ax.tick_params(axis='x', labelsize=13)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / 'test_heatmap_rfe.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved: test_heatmap_rfe.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66381705",
   "metadata": {},
   "source": [
    "## 5. Performance Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ed4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side heatmaps for enhanced and RFE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Enhanced models heatmap\n",
    "enhanced_df = df_test[df_test['model_source'] == 'enhanced']\n",
    "if len(enhanced_df) > 0:\n",
    "    pivot_enhanced = enhanced_df.pivot(index='model', columns='target', values='r2')\n",
    "    pivot_enhanced = pivot_enhanced[targets]  # Ensure consistent order\n",
    "    pivot_enhanced = pivot_enhanced.sort_values(pivot_enhanced.columns.tolist(), ascending=False)\n",
    "    \n",
    "    sns.heatmap(pivot_enhanced, annot=True, fmt='.4f', cmap='RdYlGn', center=0.35,\n",
    "                ax=axes[0], linewidths=1, vmin=0, vmax=0.9, cbar_kws={'label': 'R² Score'},\n",
    "                annot_kws={'size': 11, 'weight': 'bold'})\n",
    "    axes[0].set_title('Enhanced Models (414 features)', fontsize=16, fontweight='bold', pad=15)\n",
    "    axes[0].set_xlabel('Target Variable', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Model', fontsize=14, fontweight='bold')\n",
    "    axes[0].tick_params(axis='x', labelsize=12)\n",
    "    axes[0].tick_params(axis='y', labelsize=11)\n",
    "\n",
    "# RFE models heatmap\n",
    "rfe_df = df_test[df_test['model_source'] == 'rfe']\n",
    "if len(rfe_df) > 0:\n",
    "    pivot_rfe = rfe_df.pivot(index='model', columns='target', values='r2')\n",
    "    pivot_rfe = pivot_rfe[targets]\n",
    "    pivot_rfe = pivot_rfe.sort_values(pivot_rfe.columns.tolist(), ascending=False)\n",
    "    \n",
    "    sns.heatmap(pivot_rfe, annot=True, fmt='.4f', cmap='RdYlGn', center=0.35,\n",
    "                ax=axes[1], linewidths=1, vmin=0, vmax=0.9, cbar_kws={'label': 'R² Score'},\n",
    "                annot_kws={'size': 11, 'weight': 'bold'})\n",
    "    axes[1].set_title('RFE Models (34-394 features)', fontsize=16, fontweight='bold', pad=15)\n",
    "    axes[1].set_xlabel('Target Variable', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Model', fontsize=14, fontweight='bold')\n",
    "    axes[1].tick_params(axis='x', labelsize=12)\n",
    "    axes[1].tick_params(axis='y', labelsize=11)\n",
    "\n",
    "plt.suptitle('Test Set R² Scores - Enhanced vs RFE', fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'test_heatmap_enhanced_vs_rfe.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: test_heatmap_enhanced_vs_rfe.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27994151",
   "metadata": {},
   "source": [
    "## 6. Feature Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze R² per feature (efficiency metric)\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE EFFICIENCY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nR² per feature (higher = more efficient):\")\n",
    "print()\n",
    "\n",
    "efficiency_data = []\n",
    "\n",
    "for target in targets:\n",
    "    target_df = df_test[df_test['target'] == target]\n",
    "    \n",
    "    # Best enhanced\n",
    "    enhanced_df = target_df[target_df['model_source'] == 'enhanced']\n",
    "    if len(enhanced_df) > 0:\n",
    "        best_enhanced = enhanced_df.loc[enhanced_df['r2'].idxmax()]\n",
    "        enhanced_efficiency = best_enhanced['r2'] / best_enhanced['num_features']\n",
    "    \n",
    "    # Best RFE\n",
    "    rfe_df = target_df[target_df['model_source'] == 'rfe']\n",
    "    if len(rfe_df) > 0:\n",
    "        best_rfe = rfe_df.loc[rfe_df['r2'].idxmax()]\n",
    "        rfe_efficiency = best_rfe['r2'] / best_rfe['num_features']\n",
    "    \n",
    "    if len(enhanced_df) > 0 and len(rfe_df) > 0:\n",
    "        efficiency_data.append({\n",
    "            'target': target,\n",
    "            'enhanced_r2': best_enhanced['r2'],\n",
    "            'enhanced_features': int(best_enhanced['num_features']),\n",
    "            'enhanced_efficiency': enhanced_efficiency,\n",
    "            'rfe_r2': best_rfe['r2'],\n",
    "            'rfe_features': int(best_rfe['num_features']),\n",
    "            'rfe_efficiency': rfe_efficiency,\n",
    "            'efficiency_gain': (rfe_efficiency / enhanced_efficiency - 1) * 100\n",
    "        })\n",
    "        \n",
    "        print(f\"{target.upper()}:\")\n",
    "        print(f\"  Enhanced: R²={best_enhanced['r2']:.4f} / {int(best_enhanced['num_features'])} features = {enhanced_efficiency:.6f}\")\n",
    "        print(f\"  RFE:      R²={best_rfe['r2']:.4f} / {int(best_rfe['num_features'])} features = {rfe_efficiency:.6f}\")\n",
    "        print(f\"  Efficiency gain: {(rfe_efficiency / enhanced_efficiency - 1) * 100:+.1f}%\")\n",
    "        print()\n",
    "\n",
    "efficiency_df = pd.DataFrame(efficiency_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca623df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RFE models feature efficiency\n",
    "if len(efficiency_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    x = np.arange(len(efficiency_df))\n",
    "    \n",
    "    # Plot 1: R² with feature count\n",
    "    bars = axes[0].bar(x, efficiency_df['rfe_r2'], color='#e74c3c',\n",
    "                      alpha=0.85, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    axes[0].set_ylabel('R² Score', fontweight='bold', fontsize=14)\n",
    "    axes[0].set_xlabel('Target', fontweight='bold', fontsize=14)\n",
    "    axes[0].set_title('RFE Models Performance', fontweight='bold', fontsize=16, pad=15)\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels([t.capitalize() for t in efficiency_df['target']], fontsize=13)\n",
    "    axes[0].grid(True, alpha=0.25, linestyle='--', axis='y')\n",
    "    \n",
    "    # Add value and feature count labels\n",
    "    for i, (bar, val, feats) in enumerate(zip(bars, efficiency_df['rfe_r2'], \n",
    "                                               efficiency_df['rfe_features'])):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f\"{val:.4f}\\n({int(feats)} feat)\", ha='center', va='bottom',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Feature efficiency (R² per feature)\n",
    "    bars = axes[1].bar(x, efficiency_df['rfe_efficiency'] * 1000, color='#e74c3c',\n",
    "                      alpha=0.85, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    axes[1].set_ylabel('R² per Feature (×1000)', fontweight='bold', fontsize=14)\n",
    "    axes[1].set_xlabel('Target', fontweight='bold', fontsize=14)\n",
    "    axes[1].set_title('RFE Feature Efficiency', fontweight='bold', fontsize=16, pad=15)\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels([t.capitalize() for t in efficiency_df['target']], fontsize=13)\n",
    "    axes[1].grid(True, alpha=0.25, linestyle='--', axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, efficiency_df['rfe_efficiency'] * 1000)):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "                    f\"{val:.2f}\", ha='center', va='bottom',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / 'test_rfe_efficiency.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved: test_rfe_efficiency.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dbdb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE models ranking\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "rfe_avg = df_test[df_test['model_source'] == 'rfe'].groupby('model')['r2'].mean().sort_values(ascending=False)\n",
    "colors_bar = ['#27ae60' if '_tuned' in m else '#e74c3c' for m in rfe_avg.index]\n",
    "\n",
    "bars = ax.barh(range(len(rfe_avg)), rfe_avg.values, color=colors_bar,\n",
    "               alpha=0.85, edgecolor='black', linewidth=1.5)\n",
    "ax.set_yticks(range(len(rfe_avg)))\n",
    "ax.set_yticklabels(rfe_avg.index, fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('Average R² (across all targets)', fontweight='bold', fontsize=14)\n",
    "ax.set_title('RFE Models Performance Ranking\\n(34-394 features per target)', \n",
    "            fontweight='bold', fontsize=16, pad=15)\n",
    "ax.grid(True, alpha=0.25, linestyle='--', axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, rfe_avg.values)):\n",
    "    ax.text(val + 0.005, i, f'{val:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#e74c3c', edgecolor='black', label='Default', alpha=0.85),\n",
    "    Patch(facecolor='#27ae60', edgecolor='black', label='Tuned', alpha=0.85)\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', frameon=True,\n",
    "         fancybox=False, edgecolor='black', framealpha=0.95, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'test_rfe_ranking.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: test_rfe_ranking.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6848c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature efficiency\n",
    "if len(efficiency_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Number of features\n",
    "    x = np.arange(len(efficiency_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[0].bar(x - width/2, efficiency_df['enhanced_features'], width,\n",
    "                       label='Enhanced', color=colors_enhanced, alpha=0.85,\n",
    "                       edgecolor='black', linewidth=1.2)\n",
    "    bars2 = axes[0].bar(x + width/2, efficiency_df['rfe_features'], width,\n",
    "                       label='RFE', color=colors_rfe, alpha=0.85,\n",
    "                       edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    axes[0].set_ylabel('Number of Features', fontweight='bold')\n",
    "    axes[0].set_xlabel('Target', fontweight='bold')\n",
    "    axes[0].set_title('Feature Count: Enhanced vs RFE', fontweight='bold', fontsize=15)\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels([t.capitalize() for t in efficiency_df['target']])\n",
    "    axes[0].legend(frameon=True, fancybox=False, edgecolor='black', framealpha=0.95)\n",
    "    axes[0].grid(True, alpha=0.25, linestyle='--', axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "        axes[0].text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 5,\n",
    "                    f\"{int(bar1.get_height())}\", ha='center', va='bottom',\n",
    "                    fontsize=11, fontweight='bold')\n",
    "        axes[0].text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 5,\n",
    "                    f\"{int(bar2.get_height())}\", ha='center', va='bottom',\n",
    "                    fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Efficiency (R² per feature)\n",
    "    bars1 = axes[1].bar(x - width/2, efficiency_df['enhanced_efficiency'] * 1000, width,\n",
    "                       label='Enhanced', color=colors_enhanced, alpha=0.85,\n",
    "                       edgecolor='black', linewidth=1.2)\n",
    "    bars2 = axes[1].bar(x + width/2, efficiency_df['rfe_efficiency'] * 1000, width,\n",
    "                       label='RFE', color=colors_rfe, alpha=0.85,\n",
    "                       edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    axes[1].set_ylabel('R² per Feature (×1000)', fontweight='bold')\n",
    "    axes[1].set_xlabel('Target', fontweight='bold')\n",
    "    axes[1].set_title('Feature Efficiency: R² per Feature', fontweight='bold', fontsize=15)\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels([t.capitalize() for t in efficiency_df['target']])\n",
    "    axes[1].legend(frameon=True, fancybox=False, edgecolor='black', framealpha=0.95)\n",
    "    axes[1].grid(True, alpha=0.25, linestyle='--', axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "        axes[1].text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 0.05,\n",
    "                    f\"{bar1.get_height():.2f}\", ha='center', va='bottom',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "        axes[1].text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 0.05,\n",
    "                    f\"{bar2.get_height():.2f}\", ha='center', va='bottom',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Feature Efficiency Analysis', fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / 'test_feature_efficiency.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved: test_feature_efficiency.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd03f5c",
   "metadata": {},
   "source": [
    "## 7. Model Family Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare default vs tuned models\n",
    "print(\"=\"*80)\n",
    "print(\"DEFAULT VS TUNED MODELS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract base model name and tuning status\n",
    "df_test['model_base'] = df_test['model'].str.replace('_tuned', '')\n",
    "df_test['is_tuned'] = df_test['model'].str.contains('_tuned')\n",
    "\n",
    "tuning_comparison = df_test.groupby(['model_base', 'is_tuned', 'model_source'])['r2'].mean().reset_index()\n",
    "tuning_pivot = tuning_comparison.pivot_table(\n",
    "    index='model_base', \n",
    "    columns=['is_tuned', 'model_source'], \n",
    "    values='r2'\n",
    ").round(4)\n",
    "\n",
    "print(\"\\nAverage R² across all targets:\")\n",
    "print(tuning_pivot)\n",
    "\n",
    "# Calculate improvement from tuning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING IMPACT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_base in tuning_pivot.index:\n",
    "    print(f\"\\n{model_base}:\")\n",
    "    try:\n",
    "        if (False, 'enhanced') in tuning_pivot.columns and (True, 'enhanced') in tuning_pivot.columns:\n",
    "            default_enhanced = tuning_pivot.loc[model_base, (False, 'enhanced')]\n",
    "            tuned_enhanced = tuning_pivot.loc[model_base, (True, 'enhanced')]\n",
    "            if pd.notna(default_enhanced) and pd.notna(tuned_enhanced):\n",
    "                improvement = (tuned_enhanced - default_enhanced) / default_enhanced * 100\n",
    "                print(f\"  Enhanced: {default_enhanced:.4f} → {tuned_enhanced:.4f} ({improvement:+.2f}%)\")\n",
    "        \n",
    "        if (False, 'rfe') in tuning_pivot.columns and (True, 'rfe') in tuning_pivot.columns:\n",
    "            default_rfe = tuning_pivot.loc[model_base, (False, 'rfe')]\n",
    "            tuned_rfe = tuning_pivot.loc[model_base, (True, 'rfe')]\n",
    "            if pd.notna(default_rfe) and pd.notna(tuned_rfe):\n",
    "                improvement = (tuned_rfe - default_rfe) / default_rfe * 100\n",
    "                print(f\"  RFE:      {default_rfe:.4f} → {tuned_rfe:.4f} ({improvement:+.2f}%)\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d541e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model family performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Enhanced models\n",
    "enhanced_avg = df_test[df_test['model_source'] == 'enhanced'].groupby('model')['r2'].mean().sort_values(ascending=False)\n",
    "colors_bar = ['#27ae60' if '_tuned' in m else '#3498db' for m in enhanced_avg.index]\n",
    "\n",
    "bars = axes[0].barh(range(len(enhanced_avg)), enhanced_avg.values, color=colors_bar,\n",
    "                    alpha=0.85, edgecolor='black', linewidth=1.2)\n",
    "axes[0].set_yticks(range(len(enhanced_avg)))\n",
    "axes[0].set_yticklabels(enhanced_avg.index, fontsize=10)\n",
    "axes[0].set_xlabel('Average R² (across all targets)', fontweight='bold')\n",
    "axes[0].set_title('Enhanced Models (414 features)', fontweight='bold', fontsize=15)\n",
    "axes[0].grid(True, alpha=0.25, linestyle='--', axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, enhanced_avg.values)):\n",
    "    axes[0].text(val + 0.005, i, f'{val:.4f}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# RFE models\n",
    "rfe_avg = df_test[df_test['model_source'] == 'rfe'].groupby('model')['r2'].mean().sort_values(ascending=False)\n",
    "colors_bar = ['#27ae60' if '_tuned' in m else '#e74c3c' for m in rfe_avg.index]\n",
    "\n",
    "bars = axes[1].barh(range(len(rfe_avg)), rfe_avg.values, color=colors_bar,\n",
    "                    alpha=0.85, edgecolor='black', linewidth=1.2)\n",
    "axes[1].set_yticks(range(len(rfe_avg)))\n",
    "axes[1].set_yticklabels(rfe_avg.index, fontsize=10)\n",
    "axes[1].set_xlabel('Average R² (across all targets)', fontweight='bold')\n",
    "axes[1].set_title('RFE Models (reduced features)', fontweight='bold', fontsize=15)\n",
    "axes[1].grid(True, alpha=0.25, linestyle='--', axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, rfe_avg.values)):\n",
    "    axes[1].text(val + 0.005, i, f'{val:.4f}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#3498db', edgecolor='black', label='Default'),\n",
    "    Patch(facecolor='#27ae60', edgecolor='black', label='Tuned')\n",
    "]\n",
    "axes[0].legend(handles=legend_elements, loc='lower right', frameon=True, \n",
    "              fancybox=False, edgecolor='black', framealpha=0.95)\n",
    "axes[1].legend(handles=legend_elements, loc='lower right', frameon=True,\n",
    "              fancybox=False, edgecolor='black', framealpha=0.95)\n",
    "\n",
    "plt.suptitle('Model Performance Ranking (Average R² across targets)',\n",
    "             fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'test_model_ranking.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: test_model_ranking.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64934db",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b69fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary for thesis table\n",
    "summary_rows = []\n",
    "\n",
    "for target in targets:\n",
    "    target_df = df_test[df_test['target'] == target]\n",
    "    \n",
    "    for model_source in ['enhanced', 'rfe']:\n",
    "        source_df = target_df[target_df['model_source'] == model_source]\n",
    "        \n",
    "        if len(source_df) > 0:\n",
    "            # Get best model\n",
    "            best = source_df.loc[source_df['r2'].idxmax()]\n",
    "            \n",
    "            # Get statistics\n",
    "            summary_rows.append({\n",
    "                'Target': target.capitalize(),\n",
    "                'Model Source': 'Enhanced' if model_source == 'enhanced' else 'RFE',\n",
    "                'Num Features': int(best['num_features']),\n",
    "                'Best Model': best['model'],\n",
    "                'R²': best['r2'],\n",
    "                'RMSE': best['rmse'],\n",
    "                'MAE': best['mae'],\n",
    "                'Explained Var': best['explained_variance'],\n",
    "                'R²/Feature (×1000)': (best['r2'] / best['num_features']) * 1000\n",
    "            })\n",
    "\n",
    "summary_table = pd.DataFrame(summary_rows)\n",
    "summary_table = summary_table.round(4)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"COMPREHENSIVE SUMMARY - BEST MODELS PER TARGET AND SOURCE\")\n",
    "print(\"=\"*100)\n",
    "print(summary_table.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save to CSV\n",
    "summary_path = RESULTS_DIR / 'test_evaluation_comprehensive_summary.csv'\n",
    "summary_table.to_csv(summary_path, index=False)\n",
    "print(f\"\\nSaved: {summary_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a0d3d",
   "metadata": {},
   "source": [
    "## 9. Key Findings for Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "                       KEY FINDINGS - TEST SET EVALUATION\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "1. OVERALL PERFORMANCE:\n",
    "   • Test set: 86,453 held-out songs (never seen during training/validation)\n",
    "   • Total evaluations: {} (Enhanced + RFE)\n",
    "   • Best performing target: {} (R² = {:.4f})\n",
    "   • Most challenging target: {} (R² = {:.4f})\n",
    "\n",
    "2. ENHANCED MODELS (414 features):\n",
    "   • Features: 23 audio+artist + 5 text + 2 sentiment + 384 embeddings\n",
    "   • Average R² across all targets: {:.4f}\n",
    "   • Best overall model: {}\n",
    "   • Captures maximum information but computationally expensive\n",
    "\n",
    "3. RFE MODELS (reduced features):\n",
    "   • Feature reduction: 34-394 features per target ({}% avg reduction)\n",
    "   • Average R² across all targets: {:.4f}\n",
    "   • Performance trade-off: {:.2f}% avg R² loss for {}% fewer features\n",
    "   • Significant efficiency gain: {}x R² per feature improvement\n",
    "\n",
    "4. MODEL FAMILY INSIGHTS:\n",
    "   • Gradient boosting (CatBoost, LightGBM, XGBoost) dominate performance\n",
    "   • Hyperparameter tuning provides consistent improvements\n",
    "   • Neural networks (MLPRegressor) competitive but require more tuning\n",
    "   • Tree ensembles (ExtraTrees, RandomForest) solid baselines\n",
    "\n",
    "5. FEATURE SELECTION IMPACT:\n",
    "   • RFE successfully identifies most informative features\n",
    "   • Energy prediction: 92% feature reduction, minimal R² loss\n",
    "   • Danceability: 82% feature reduction, maintains strong performance\n",
    "   • Popularity: Requires most features (minimal reduction effective)\n",
    "\n",
    "6. PRACTICAL IMPLICATIONS:\n",
    "   • RFE models recommended for production (faster inference, similar accuracy)\n",
    "   • Enhanced models useful when maximum accuracy needed\n",
    "   • Artist features contribute meaningfully (Experiment 2 improvement)\n",
    "   • Target-specific feature selection crucial for efficiency\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "\"\"\".format(\n",
    "    len(df_test),\n",
    "    df_test.loc[df_test['r2'].idxmax(), 'target'].capitalize(),\n",
    "    df_test['r2'].max(),\n",
    "    df_test.loc[df_test['r2'].idxmin(), 'target'].capitalize(),\n",
    "    df_test['r2'].min(),\n",
    "    df_test[df_test['model_source'] == 'enhanced']['r2'].mean(),\n",
    "    df_test[df_test['model_source'] == 'enhanced'].loc[df_test[df_test['model_source'] == 'enhanced']['r2'].idxmax(), 'model'],\n",
    "    (1 - df_test[df_test['model_source'] == 'rfe']['num_features'].mean() / 414) * 100,\n",
    "    df_test[df_test['model_source'] == 'rfe']['r2'].mean(),\n",
    "    (df_test[df_test['model_source'] == 'enhanced']['r2'].mean() - df_test[df_test['model_source'] == 'rfe']['r2'].mean()) / df_test[df_test['model_source'] == 'enhanced']['r2'].mean() * 100,\n",
    "    (1 - df_test[df_test['model_source'] == 'rfe']['num_features'].mean() / 414) * 100,\n",
    "    (df_test[df_test['model_source'] == 'rfe']['r2'].mean() / df_test[df_test['model_source'] == 'rfe']['num_features'].mean()) / (df_test[df_test['model_source'] == 'enhanced']['r2'].mean() / 414)\n",
    "))\n",
    "\n",
    "print(\"\\nAll figures saved to:\", FIGURES_DIR)\n",
    "print(\"\\nFigures created:\")\n",
    "for fig_file in sorted(FIGURES_DIR.glob('test_*.png')):\n",
    "    print(f\"   {fig_file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST EVALUATION ANALYSIS COMPLETE - Ready for thesis integration\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
