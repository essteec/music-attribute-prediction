{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281687eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Academic publication style with larger fonts for readability\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman', 'DejaVu Serif', 'serif'],\n",
    "    'font.size': 13,  # Increased from 11\n",
    "    'axes.titlesize': 15,  # Increased from 12\n",
    "    'axes.labelsize': 14,  # Increased from 11\n",
    "    'xtick.labelsize': 12,  # Increased from 10\n",
    "    'ytick.labelsize': 12,  # Increased from 10\n",
    "    'legend.fontsize': 12,  # Increased from 10\n",
    "    'figure.titlesize': 16,  # Increased from 14\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': False,\n",
    "    'grid.alpha': 0.3,\n",
    "    'axes.linewidth': 1.2,  # Thicker axis lines\n",
    "    'xtick.major.width': 1.2,\n",
    "    'ytick.major.width': 1.2,\n",
    "})\n",
    "\n",
    "# Paths - Experiment 2 (with artist features)\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "FEATURES_DIR = PROJECT_ROOT / 'features'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models' / 'saved' / 'experiment2_with_artist'\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "FIGURES_DIR = PROJECT_ROOT / 'results' / 'figures'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Figures will be saved to: {FIGURES_DIR}\")\n",
    "print(f\"\\nExperiment 2: 414 features (23 audio+artist + 5 text + 2 sentiment + 384 embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff70384",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test features - Experiment 2 (with artist data)\n",
    "# Note: X_test_audio.npy contains 23 features (21 audio + 2 artist combined)\n",
    "X_test_audio = np.load(FEATURES_DIR / 'X_test_audio.npy')\n",
    "X_test_text_stats = np.load(FEATURES_DIR / 'X_test_text_stats.npy')\n",
    "X_test_sentiment = np.load(FEATURES_DIR / 'X_test_sentiment.npy')\n",
    "X_test_embeddings = np.load(FEATURES_DIR / 'X_test_embeddings.npy')\n",
    "\n",
    "# Combine features: 23 audio+artist + 5 text + 2 sentiment + 384 embeddings = 414\n",
    "X_test = np.hstack([X_test_audio, X_test_text_stats, X_test_sentiment, X_test_embeddings])\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"  Audio+Artist: {X_test_audio.shape[1]} features\")\n",
    "print(f\"  Text Stats: {X_test_text_stats.shape[1]} features\")\n",
    "print(f\"  Sentiment: {X_test_sentiment.shape[1]} features\")\n",
    "print(f\"  Embeddings: {X_test_embeddings.shape[1]} features\")\n",
    "print(f\"  Total: {X_test.shape[1]} features\")\n",
    "\n",
    "# Load targets\n",
    "TARGETS = ['valence', 'energy', 'danceability', 'popularity']\n",
    "y_test = {}\n",
    "for target in TARGETS:\n",
    "    y_test[target] = np.load(FEATURES_DIR / f'y_test_{target}.npy')\n",
    "    print(f\"y_test_{target}: {y_test[target].shape}\")\n",
    "\n",
    "# Load test CSV for metadata (genre, year)\n",
    "df_test = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "print(f\"\\nTest DataFrame shape: {df_test.shape}\")\n",
    "print(f\"Test samples: {len(df_test):,} songs\")\n",
    "print(f\"Columns available: {df_test.columns.tolist()[:15]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best models per target (Experiment 2 - will be determined from test evaluation)\n",
    "# Using likely best performers based on Exp 2 validation results\n",
    "BEST_MODELS = {\n",
    "    'valence': 'XGBoost_tuned',\n",
    "    'energy': 'XGBoost_tuned',\n",
    "    'danceability': 'XGBoost_tuned',\n",
    "    'popularity': 'CatBoost'\n",
    "}\n",
    "\n",
    "print(\"Loading Experiment 2 best models...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load best models and generate predictions\n",
    "models = {}\n",
    "predictions = {}\n",
    "residuals = {}\n",
    "\n",
    "for target, model_name in BEST_MODELS.items():\n",
    "    model_path = MODELS_DIR / f\"{model_name}_{target}.pkl\"\n",
    "    if model_path.exists():\n",
    "        models[target] = joblib.load(model_path)\n",
    "        predictions[target] = models[target].predict(X_test)\n",
    "        residuals[target] = y_test[target] - predictions[target]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test[target], predictions[target]))\n",
    "        mae = mean_absolute_error(y_test[target], predictions[target])\n",
    "        r2 = r2_score(y_test[target], predictions[target])\n",
    "        print(f\"{target:12s}: {model_name:20s} (R²={r2:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f})\")\n",
    "    else:\n",
    "        print(f\"WARNING: Model not found: {model_path}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444b926",
   "metadata": {},
   "source": [
    "## 2. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651891c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_actual(y_true, y_pred, target_name, ax=None):\n",
    "    \"\"\"Scatter plot of predicted vs actual values with identity line.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # Subsample for clarity if too many points\n",
    "    n_samples = len(y_true)\n",
    "    if n_samples > 10000:\n",
    "        idx = np.random.choice(n_samples, 10000, replace=False)\n",
    "        y_true_plot = y_true[idx]\n",
    "        y_pred_plot = y_pred[idx]\n",
    "    else:\n",
    "        y_true_plot = y_true\n",
    "        y_pred_plot = y_pred\n",
    "    \n",
    "    ax.scatter(y_true_plot, y_pred_plot, alpha=0.2, s=12, c='#2c3e50', edgecolors='none')\n",
    "    \n",
    "    # Identity line\n",
    "    lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n",
    "    ax.plot(lims, lims, 'r--', lw=2, label='Perfect prediction', alpha=0.8)\n",
    "    \n",
    "    # Metrics\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    ax.set_xlabel(f'Actual {target_name.capitalize()}', fontweight='bold')\n",
    "    ax.set_ylabel(f'Predicted {target_name.capitalize()}', fontweight='bold')\n",
    "    ax.set_title(f'{target_name.capitalize()}\\n$R^2$={r2:.3f}, RMSE={rmse:.3f}', \n",
    "                 fontweight='bold', pad=12)\n",
    "    ax.legend(loc='lower right', frameon=True, fancybox=False, \n",
    "              edgecolor='black', framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.2, linestyle='--')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual for all targets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    if target in predictions:\n",
    "        plot_predicted_vs_actual(y_test[target], predictions[target], target, axes[i])\n",
    "\n",
    "plt.suptitle('Predicted vs Actual Values', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'exp2_error_predicted_vs_actual.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'exp2_error_predicted_vs_actual.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9360a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_distribution(residuals, target_name, ax=None):\n",
    "    \"\"\"Histogram of prediction errors (residuals).\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    \n",
    "    ax.hist(residuals, bins=60, color='#3498db', edgecolor='white', alpha=0.85, linewidth=0.5)\n",
    "    ax.axvline(0, color='red', linestyle='--', lw=2, label='Zero error', alpha=0.8)\n",
    "    ax.axvline(residuals.mean(), color='orange', linestyle='-', lw=2, \n",
    "               label=f'Mean={residuals.mean():.4f}', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Prediction Error (Actual - Predicted)', fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax.set_title(f'{target_name.capitalize()} Error Distribution\\nStd={residuals.std():.4f}', \n",
    "                 fontweight='bold', pad=12)\n",
    "    ax.legend(loc='upper right', frameon=True, fancybox=False, \n",
    "              edgecolor='black', framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.2, linestyle='--', axis='y')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b07cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution for all targets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    if target in residuals:\n",
    "        plot_error_distribution(residuals[target], target, axes[i])\n",
    "\n",
    "plt.suptitle('Experiment 2: Error Distribution', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'exp2_error_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'exp2_error_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985efff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residual_vs_predicted(y_pred, residuals, target_name, ax=None):\n",
    "    \"\"\"Residual plot to check for heteroscedasticity.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    \n",
    "    # Subsample for clarity\n",
    "    n_samples = len(y_pred)\n",
    "    if n_samples > 10000:\n",
    "        idx = np.random.choice(n_samples, 10000, replace=False)\n",
    "        y_pred_plot = y_pred[idx]\n",
    "        res_plot = residuals[idx]\n",
    "    else:\n",
    "        y_pred_plot = y_pred\n",
    "        res_plot = residuals\n",
    "    \n",
    "    ax.scatter(y_pred_plot, res_plot, alpha=0.2, s=12, c='#2c3e50', edgecolors='none')\n",
    "    ax.axhline(0, color='red', linestyle='--', lw=2, alpha=0.8)\n",
    "    \n",
    "    # Add smoothed trend line\n",
    "    sorted_idx = np.argsort(y_pred)\n",
    "    window = len(y_pred) // 20\n",
    "    if window > 10:\n",
    "        rolling_mean = pd.Series(residuals[sorted_idx]).rolling(window, center=True).mean()\n",
    "        ax.plot(y_pred[sorted_idx], rolling_mean, color='orange', lw=2.5, \n",
    "                label='Trend', alpha=0.9)\n",
    "        ax.legend(loc='best', frameon=True, fancybox=False, \n",
    "                  edgecolor='black', framealpha=0.9)\n",
    "    \n",
    "    ax.set_xlabel(f'Predicted {target_name.capitalize()}', fontweight='bold')\n",
    "    ax.set_ylabel('Residuals', fontweight='bold')\n",
    "    ax.set_title(f'{target_name.capitalize()} Residuals', \n",
    "                 fontweight='bold', pad=12)\n",
    "    ax.grid(True, alpha=0.2, linestyle='--')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03901b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots for all targets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    if target in predictions and target in residuals:\n",
    "        plot_residual_vs_predicted(predictions[target], residuals[target], target, axes[i])\n",
    "\n",
    "plt.suptitle('Experiment 2: Residual Analysis', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'exp2_error_residual_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'exp2_error_residual_plots.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca96b1",
   "metadata": {},
   "source": [
    "## 3. Error Segmentation by Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check genre column\n",
    "if 'genre' in df_test.columns:\n",
    "    print(\"Genre distribution in test set:\")\n",
    "    print(df_test['genre'].value_counts())\n",
    "else:\n",
    "    print(\"Available columns:\", df_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e733076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_by_genre(df, y_true, y_pred, target_name):\n",
    "    \"\"\"Calculate error metrics by genre.\"\"\"\n",
    "    if 'genre' not in df.columns:\n",
    "        print(\"Genre column not found\")\n",
    "        return None\n",
    "    \n",
    "    # Create analysis dataframe\n",
    "    analysis_df = pd.DataFrame({\n",
    "        'genre': df['genre'].values[:len(y_true)],\n",
    "        'actual': y_true,\n",
    "        'predicted': y_pred,\n",
    "        'error': y_true - y_pred,\n",
    "        'abs_error': np.abs(y_true - y_pred),\n",
    "        'squared_error': (y_true - y_pred) ** 2\n",
    "    })\n",
    "    \n",
    "    # Aggregate by genre\n",
    "    genre_stats = analysis_df.groupby('genre').agg({\n",
    "        'actual': ['count', 'mean', 'std'],\n",
    "        'error': ['mean', 'std'],\n",
    "        'abs_error': 'mean',\n",
    "        'squared_error': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    genre_stats.columns = ['count', 'mean_actual', 'std_actual', \n",
    "                           'mean_error', 'std_error', 'mae', 'mse']\n",
    "    genre_stats['rmse'] = np.sqrt(genre_stats['mse'])\n",
    "    genre_stats = genre_stats.sort_values('rmse', ascending=False)\n",
    "    \n",
    "    return analysis_df, genre_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4375921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by genre for each target\n",
    "genre_analysis = {}\n",
    "genre_stats = {}\n",
    "\n",
    "for target in TARGETS:\n",
    "    if target in predictions:\n",
    "        result = analyze_error_by_genre(df_test, y_test[target], predictions[target], target)\n",
    "        if result is not None:\n",
    "            genre_analysis[target], genre_stats[target] = result\n",
    "            print(f\"\\n{target.upper()} - Error by Genre:\")\n",
    "            print(genre_stats[target][['count', 'mae', 'rmse', 'mean_error']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baedd65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error by genre\n",
    "if genre_stats:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 11))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, 10))\n",
    "    \n",
    "    for i, target in enumerate(TARGETS):\n",
    "        if target in genre_stats:\n",
    "            stats = genre_stats[target].sort_values('rmse', ascending=True)\n",
    "            \n",
    "            bars = axes[i].barh(range(len(stats)), stats['rmse'], color=colors, \n",
    "                               edgecolor='white', linewidth=1.5)\n",
    "            axes[i].set_yticks(range(len(stats)))\n",
    "            axes[i].set_yticklabels(stats.index, fontweight='bold')\n",
    "            axes[i].set_xlabel('RMSE', fontweight='bold')\n",
    "            axes[i].set_title(f'{target.capitalize()} - RMSE by Genre', \n",
    "                            fontweight='bold', pad=12)\n",
    "            axes[i].grid(True, alpha=0.2, linestyle='--', axis='x')\n",
    "            \n",
    "            # Add value labels\n",
    "            for j, (idx, row) in enumerate(stats.iterrows()):\n",
    "                axes[i].text(row['rmse'] + 0.003, j, f\"{row['rmse']:.3f}\", \n",
    "                           va='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Experiment 2: Error by Genre', \n",
    "                 fontsize=18, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / 'exp2_error_by_genre.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'exp2_error_by_genre.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee95dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of errors by genre\n",
    "if genre_analysis:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 11))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, target in enumerate(TARGETS):\n",
    "        if target in genre_analysis:\n",
    "            df_plot = genre_analysis[target]\n",
    "            \n",
    "            # Order by median error\n",
    "            order = df_plot.groupby('genre')['abs_error'].median().sort_values().index\n",
    "            \n",
    "            sns.boxplot(data=df_plot, x='genre', y='error', order=order, \n",
    "                       ax=axes[i], palette='RdYlGn_r', showfliers=False, linewidth=1.5)\n",
    "            axes[i].axhline(0, color='red', linestyle='--', lw=2, alpha=0.8)\n",
    "            axes[i].set_xlabel('Genre', fontweight='bold')\n",
    "            axes[i].set_ylabel('Prediction Error', fontweight='bold')\n",
    "            axes[i].set_title(f'{target.capitalize()} - Error Distribution by Genre', \n",
    "                            fontweight='bold', pad=12)\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            axes[i].grid(True, alpha=0.2, linestyle='--', axis='y')\n",
    "    \n",
    "    plt.suptitle('Experiment 2: Error Distribution by Genre', \n",
    "                 fontsize=18, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / 'exp2_error_boxplot_by_genre.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'exp2_error_boxplot_by_genre.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d43bcd",
   "metadata": {},
   "source": [
    "## 4. Error Segmentation by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_by_year(df, y_true, y_pred, target_name):\n",
    "    \"\"\"Calculate error metrics by release year (binned into decades).\"\"\"\n",
    "    year_col = None\n",
    "    for col in ['year', 'release_year', 'Year']:\n",
    "        if col in df.columns:\n",
    "            year_col = col\n",
    "            break\n",
    "    \n",
    "    if year_col is None:\n",
    "        print(\"Year column not found\")\n",
    "        return None\n",
    "    \n",
    "    # Create decade bins\n",
    "    years = df[year_col].values[:len(y_true)]\n",
    "    decades = (years // 10) * 10\n",
    "    \n",
    "    analysis_df = pd.DataFrame({\n",
    "        'year': years,\n",
    "        'decade': decades,\n",
    "        'actual': y_true,\n",
    "        'predicted': y_pred,\n",
    "        'abs_error': np.abs(y_true - y_pred),\n",
    "        'squared_error': (y_true - y_pred) ** 2\n",
    "    })\n",
    "    \n",
    "    # Aggregate by decade\n",
    "    decade_stats = analysis_df.groupby('decade').agg({\n",
    "        'year': 'count',\n",
    "        'abs_error': 'mean',\n",
    "        'squared_error': 'mean'\n",
    "    }).rename(columns={'year': 'count', 'abs_error': 'mae', 'squared_error': 'mse'})\n",
    "    decade_stats['rmse'] = np.sqrt(decade_stats['mse'])\n",
    "    \n",
    "    return analysis_df, decade_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by year/decade\n",
    "year_analysis = {}\n",
    "year_stats = {}\n",
    "\n",
    "for target in TARGETS:\n",
    "    if target in predictions:\n",
    "        result = analyze_error_by_year(df_test, y_test[target], predictions[target], target)\n",
    "        if result is not None:\n",
    "            year_analysis[target], year_stats[target] = result\n",
    "            print(f\"\\n{target.upper()} - Error by Decade:\")\n",
    "            print(year_stats[target].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error trends by decade\n",
    "if year_stats:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, target in enumerate(TARGETS):\n",
    "        if target in year_stats:\n",
    "            stats = year_stats[target]\n",
    "            # Filter to decades with sufficient samples\n",
    "            stats = stats[stats['count'] >= 100]\n",
    "            \n",
    "            bars = axes[i].bar(stats.index.astype(str), stats['rmse'], \n",
    "                              color='#3498db', edgecolor='white', alpha=0.85, linewidth=1.5)\n",
    "            axes[i].set_xlabel('Decade', fontweight='bold')\n",
    "            axes[i].set_ylabel('RMSE', fontweight='bold')\n",
    "            axes[i].set_title(f'{target.capitalize()} - RMSE by Decade', \n",
    "                            fontweight='bold', pad=12)\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            axes[i].grid(True, alpha=0.2, linestyle='--', axis='y')\n",
    "            \n",
    "            # Add count labels\n",
    "            for j, (idx, row) in enumerate(stats.iterrows()):\n",
    "                axes[i].text(j, row['rmse'] + 0.008, f\"n={int(row['count'])}\", \n",
    "                           ha='center', fontsize=10, rotation=90, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Experiment 2: Error by Decade', \n",
    "                 fontsize=18, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / 'exp2_error_by_decade.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'exp2_error_by_decade.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6804f3",
   "metadata": {},
   "source": [
    "## 5. Error Segmentation by Target Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_by_target_range(y_true, y_pred, target_name, n_bins=5):\n",
    "    \"\"\"Analyze errors across different ranges of the target variable.\"\"\"\n",
    "    \n",
    "    # Create bins based on actual values\n",
    "    bin_edges = np.percentile(y_true, np.linspace(0, 100, n_bins + 1))\n",
    "    bin_labels = [f\"{bin_edges[i]:.2f}-{bin_edges[i+1]:.2f}\" for i in range(n_bins)]\n",
    "    \n",
    "    bins = np.digitize(y_true, bin_edges[1:-1])\n",
    "    \n",
    "    analysis_df = pd.DataFrame({\n",
    "        'bin': bins,\n",
    "        'bin_label': [bin_labels[b] for b in bins],\n",
    "        'actual': y_true,\n",
    "        'predicted': y_pred,\n",
    "        'error': y_true - y_pred,\n",
    "        'abs_error': np.abs(y_true - y_pred)\n",
    "    })\n",
    "    \n",
    "    # Aggregate by bin\n",
    "    bin_stats = analysis_df.groupby('bin').agg({\n",
    "        'actual': ['count', 'mean'],\n",
    "        'error': 'mean',\n",
    "        'abs_error': 'mean'\n",
    "    })\n",
    "    bin_stats.columns = ['count', 'mean_actual', 'mean_error', 'mae']\n",
    "    bin_stats['rmse'] = analysis_df.groupby('bin').apply(\n",
    "        lambda x: np.sqrt(np.mean((x['actual'] - x['predicted'])**2))\n",
    "    )\n",
    "    \n",
    "    # Map bin labels based on actual bin indices present in the data\n",
    "    bin_stats['bin_label'] = bin_stats.index.map(lambda x: bin_labels[x])\n",
    "    \n",
    "    return analysis_df, bin_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb4217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by target range\n",
    "range_analysis = {}\n",
    "range_stats = {}\n",
    "\n",
    "for target in TARGETS:\n",
    "    if target in predictions:\n",
    "        analysis_df, stats = analyze_error_by_target_range(\n",
    "            y_test[target], predictions[target], target, n_bins=5\n",
    "        )\n",
    "        range_analysis[target] = analysis_df\n",
    "        range_stats[target] = stats\n",
    "        \n",
    "        print(f\"\\n{target.upper()} - Error by {target.capitalize()} Range:\")\n",
    "        print(stats[['count', 'mean_actual', 'mae', 'rmse', 'mean_error', 'bin_label']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error by target range\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 11))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    if target in range_stats:\n",
    "        stats = range_stats[target]\n",
    "        \n",
    "        x = range(len(stats))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = axes[i].bar([xi - width/2 for xi in x], stats['mae'], width, \n",
    "                           label='MAE', color='#3498db', alpha=0.85, \n",
    "                           edgecolor='white', linewidth=1.5)\n",
    "        bars2 = axes[i].bar([xi + width/2 for xi in x], stats['rmse'], width, \n",
    "                           label='RMSE', color='#e74c3c', alpha=0.85, \n",
    "                           edgecolor='white', linewidth=1.5)\n",
    "        \n",
    "        axes[i].set_xticks(x)\n",
    "        axes[i].set_xticklabels(stats['bin_label'], rotation=35, ha='right')\n",
    "        axes[i].set_xlabel(f'{target.capitalize()} Range', fontweight='bold')\n",
    "        axes[i].set_ylabel('Error', fontweight='bold')\n",
    "        axes[i].set_title(f'{target.capitalize()} - Error by Value Range', \n",
    "                         fontweight='bold', pad=12)\n",
    "        axes[i].legend(frameon=True, fancybox=False, edgecolor='black', framealpha=0.9)\n",
    "        axes[i].grid(True, alpha=0.2, linestyle='--', axis='y')\n",
    "\n",
    "plt.suptitle('Experiment 2: Error by Target Value Range', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'exp2_error_by_target_range.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'exp2_error_by_target_range.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias analysis: Does model over/under-predict at extremes?\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    if target in range_stats:\n",
    "        stats = range_stats[target]\n",
    "        \n",
    "        colors = ['#e74c3c' if e < 0 else '#27ae60' for e in stats['mean_error']]\n",
    "        \n",
    "        bars = axes[i].bar(range(len(stats)), stats['mean_error'], \n",
    "                          color=colors, alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "        axes[i].axhline(0, color='black', linestyle='-', lw=1.5)\n",
    "        axes[i].set_xticks(range(len(stats)))\n",
    "        axes[i].set_xticklabels(stats['bin_label'], rotation=35, ha='right')\n",
    "        axes[i].set_xlabel(f'{target.capitalize()} Range', fontweight='bold')\n",
    "        axes[i].set_ylabel('Mean Error (Actual - Predicted)', fontweight='bold')\n",
    "        axes[i].set_title(f'{target.capitalize()} - Prediction Bias by Range\\n(Positive = Under-prediction)', \n",
    "                         fontweight='bold', pad=12)\n",
    "        axes[i].grid(True, alpha=0.2, linestyle='--', axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for j, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            axes[i].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{height:.3f}', ha='center', va='bottom' if height > 0 else 'top',\n",
    "                        fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Experiment 2: Prediction Bias Analysis', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'exp2_error_bias_by_range.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'exp2_error_bias_by_range.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9728a1",
   "metadata": {},
   "source": [
    "## 6. Failure Case Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_worst_predictions(df, y_true, y_pred, target_name, n_worst=20):\n",
    "    \"\"\"Identify and analyze the worst predictions.\"\"\"\n",
    "    \n",
    "    abs_errors = np.abs(y_true - y_pred)\n",
    "    worst_idx = np.argsort(abs_errors)[-n_worst:][::-1]\n",
    "    \n",
    "    # Get song info for worst predictions\n",
    "    worst_df = df.iloc[worst_idx].copy()\n",
    "\n",
    "    # Prefer an explicit id column if present, otherwise use the original index\n",
    "    id_col = next((c for c in ['id', 'track_id', 'song_id'] if c in df.columns), None)\n",
    "    if id_col:\n",
    "        worst_df['id'] = df.iloc[worst_idx][id_col].values\n",
    "    else:\n",
    "        worst_df['id'] = worst_df.index  # fallback to the DataFrame index\n",
    "    \n",
    "\n",
    "    worst_df['actual'] = y_true[worst_idx]\n",
    "    worst_df['predicted'] = y_pred[worst_idx]\n",
    "    worst_df['error'] = y_true[worst_idx] - y_pred[worst_idx]\n",
    "    worst_df['abs_error'] = abs_errors[worst_idx]\n",
    "    \n",
    "    # Select relevant columns\n",
    "    cols_to_show = ['id', 'actual', 'predicted', 'error', 'abs_error']\n",
    "    for col in ['name', 'track_name', 'title', 'artist', 'genre', 'year']:\n",
    "        if col in worst_df.columns:\n",
    "            cols_to_show.append(col)\n",
    "    \n",
    "    return worst_df[cols_to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify worst predictions for each target\n",
    "worst_predictions = {}\n",
    "\n",
    "for target in TARGETS:\n",
    "    if target in predictions:\n",
    "        worst_df = identify_worst_predictions(\n",
    "            df_test, y_test[target], predictions[target], target, n_worst=15\n",
    "        )\n",
    "        worst_predictions[target] = worst_df\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{target.upper()} - WORST PREDICTIONS (Top 15)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        display_cols = ['id', 'actual', 'predicted', 'abs_error']\n",
    "        if 'genre' in worst_df.columns:\n",
    "            display_cols.append('genre')\n",
    "        print(worst_df[display_cols].head(15).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4fc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze characteristics of worst predictions\n",
    "def analyze_failure_characteristics(worst_df, all_df, target_name):\n",
    "    \"\"\"Compare characteristics of worst predictions vs overall dataset.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{target_name.upper()} - Failure Case Characteristics:\")\n",
    "    \n",
    "    # Genre distribution in failures vs overall\n",
    "    if 'genre' in worst_df.columns and 'genre' in all_df.columns:\n",
    "        print(\"\\nGenre distribution in worst predictions:\")\n",
    "        failure_genres = worst_df['genre'].value_counts(normalize=True)\n",
    "        overall_genres = all_df['genre'].value_counts(normalize=True)\n",
    "        \n",
    "        comparison = pd.DataFrame({\n",
    "            'Failures (%)': (failure_genres * 100).round(1),\n",
    "            'Overall (%)': (overall_genres * 100).round(1)\n",
    "        }).fillna(0)\n",
    "        comparison['Overrepresented'] = comparison['Failures (%)'] > comparison['Overall (%)'] * 1.5\n",
    "        print(comparison.to_string())\n",
    "        \n",
    "        overrep_genres = comparison[comparison['Overrepresented']].index.tolist()\n",
    "        if overrep_genres:\n",
    "            print(f\"\\nOverrepresented genres in failures: {overrep_genres}\")\n",
    "    \n",
    "    # Actual value distribution in failures\n",
    "    print(f\"\\nActual {target_name} in failures vs overall:\")\n",
    "    print(f\"  Failures: mean={worst_df['actual'].mean():.3f}, std={worst_df['actual'].std():.3f}\")\n",
    "\n",
    "for target in TARGETS:\n",
    "    if target in worst_predictions:\n",
    "        analyze_failure_characteristics(worst_predictions[target], df_test, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213554b8",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for target in TARGETS:\n",
    "    if target in predictions:\n",
    "        y_true = y_test[target]\n",
    "        y_pred = predictions[target]\n",
    "        res = residuals[target]\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Target': target.capitalize(),\n",
    "            'Model': BEST_MODELS[target],\n",
    "            'R²': r2_score(y_true, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'MAE': mean_absolute_error(y_true, y_pred),\n",
    "            'Mean Error': res.mean(),\n",
    "            'Std Error': res.std(),\n",
    "            'Max |Error|': np.abs(res).max(),\n",
    "            'Error Skewness': pd.Series(res).skew(),\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"EXPERIMENT 2 - ERROR ANALYSIS SUMMARY (WITH ARTIST FEATURES)\")\n",
    "print(\"=\"*90)\n",
    "print(summary_df.round(4).to_string(index=False))\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to CSV\n",
    "results_dir = PROJECT_ROOT / 'results' / 'metrics' / 'experiment2_with_artist'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "summary_df.to_csv(results_dir / 'error_analysis_summary.csv', index=False)\n",
    "print(f\"\\nSaved summary to: {results_dir / 'error_analysis_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d600d760",
   "metadata": {},
   "source": [
    "## 8. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e93447",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nFigures saved to:\", FIGURES_DIR)\n",
    "print(\"\\nFiles created:\")\n",
    "for f in sorted(FIGURES_DIR.glob('exp2_error_*.png')):\n",
    "    print(f\"  {f.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"ERROR ANALYSIS COMPLETE - Ready for thesis integration\")\n",
    "print(\"=\"*90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
